# -*- coding: utf-8 -*-
"""
è§£é‡Šæ€§æ³¨é‡Šç‰ˆï¼šEncoder / Decoder / VMFMM
================================================
æœ¬æ–‡ä»¶å¯¹ç”¨æˆ·æä¾›çš„ PyTorch å®ç°åŠ å…¥äº†**é€è¡Œä¸­æ–‡æ³¨é‡Š**ï¼Œå¹¶è¡¥å……äº†
è®¾è®¡åŸç†è¯´æ˜ï¼Œä¾¿äºåç»­é˜…è¯»ã€ä¿®æ”¹å’Œè°ƒè¯•ã€‚é‡ç‚¹è¯´æ˜å¦‚ä¸‹ï¼š

1. **Von Misesâ€“Fisher (vMF) åˆ†å¸ƒ**
   - ç”¨äºå»ºæ¨¡å•ä½è¶…çƒé¢ä¸Šçš„æ–¹å‘æ•°æ®ï¼ˆembedding å•ä½åŒ–åä½äº S^{D-1} ä¸Šï¼‰ã€‚
   - å‚æ•° `mu` è¡¨ç¤ºå‡å€¼æ–¹å‘ï¼›`kappa` (æ­¤å¤„å˜é‡åä¸º *k*) æ§åˆ¶é›†ä¸­åº¦ï¼Œæ•°å€¼è¶Šå¤§è¶Šé›†ä¸­ã€‚

2. **Encoder**
   - è¾“å‡º `(mu, kappa)` å‚æ•°ï¼Œå†ä» vMF åˆ†å¸ƒä¸­ *å¯å¾®åˆ†* åœ°é‡å‚æ•°åŒ–é‡‡æ ·å¾—åˆ°éšå˜é‡ *z*ã€‚
   - é‡‡æ ·çš„ *z* ä¸èšç±»ä¸­å¿ƒåŒæ ·ä½äºå•ä½çƒé¢ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨ vMF æ··åˆæ¨¡å‹è¿›è¡Œèšç±»ã€‚

3. **VMFMM (vMF Mixture Model)**
   - ç±»ä¼¼é«˜æ–¯æ··åˆæ¨¡å‹ (GMM)ï¼Œä½†ç»„ä»¶åˆ†å¸ƒæ›´æ¢ä¸º vMFï¼Œä»¥æ›´å¥‘åˆæ–¹å‘æ•°æ®ã€‚
   - è¿™é‡Œçš„ `vmfmm_Loss` å³å¯¹åº” ELBO ä¸­å…³äºèšç±»å…ˆéªŒéƒ¨åˆ†çš„å¯¹æ•°ä¼¼ç„¶ä¸Šç•Œæ¨å¯¼ã€‚

4. **åˆå§‹åŒ–ä¸è½¯é™åˆ¶**
   - ä½¿ç”¨ `softplus + r` ä¿è¯ kappa â‰¥ r > 0ï¼Œé¿å…è¿‡åˆ†å°–é”çš„é›†ä¸­åº¦å¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚
   - å¯¹ä¸­å¿ƒ `mu` åš *unitâ€‘norm* æ­£åˆ™åŒ–ï¼Œä¿è¯å…¶ä¾ç„¶è½åœ¨å•ä½çƒé¢ä¸Šã€‚

**æ³¨æ„ï¼š**æ‰€æœ‰æ³¨é‡Šä»¥ `# ä¸­æ–‡æ³¨é‡Š â€¦` æˆ–å¤šè¡Œ docstring çš„å½¢å¼ç›´æ¥åµŒå…¥ä»£ç ã€‚
å¦‚éœ€è¿›ä¸€æ­¥ç»†åŒ–ï¼Œè¯·åœ¨ ChatGPT å¯¹è¯ä¸­æŒ‡å‡ºå¾…è¡¥å……çš„è¡Œå·æˆ–å…³é”®è¯ã€‚
"""
try:
    import os
    import sys
    import math

    import numpy as np
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    # ----------------------------- é¡¹ç›®å†…éƒ¨ä¾èµ– -----------------------------
    from dsvae.utils import init_weights, d_besseli, besseli# æƒé‡åˆå§‹åŒ–å·¥å…·ï¼Œé»˜è®¤ï¼šæ­£æ€åˆ†å¸ƒ + Kaiming å˜ä½“  # ç¬¬ä¸€ç±»ä¿®æ­£è´å¡å°”å‡½æ•° I_v(x) çš„ä¸€é˜¶å¯¼æ•°ï¼Œç”¨äºæœŸæœ›è®¡ç®—# ç¬¬ä¸€ç±»ä¿®æ­£è´å¡å°”å‡½æ•° I_v(x)
    from dsvae.config import DEVICE# è¯»å–å…¨å±€è®¾å¤‡é…ç½® ("cuda" / "cpu")
    from vmfmix.von_mises_fisher import VonMisesFisher, HypersphericalUniform # å¯é‡å‚æ•°åŒ– vMF åˆ†å¸ƒå®ç° # å•ä½çƒé¢ä¸Šçš„å‡åŒ€åˆ†å¸ƒï¼ˆæ­¤å¤„æœªç”¨åˆ°ï¼‰

except ImportError as e:
    # æ•è·å¯¼å…¥é”™è¯¯ï¼Œå¹¶æŠ›å‡ºæ›´å‹å¥½çš„ä¿¡æ¯
    print(e)
    raise ImportError


class Reshape(nn.Module):
    """
    Class for performing a reshape as a layer in a sequential model.
    åœ¨ `nn.Sequential` ä¸­æ’å…¥è§†å›¾å˜æ¢å±‚çš„å·¥å…·ç±»
    """

    def __init__(self, shape=[]):
        """å‚æ•°
        ----------
        shape : tuple æˆ– list
            ç›®æ ‡å½¢çŠ¶ (ä¸åŒ…å« batch ç»´)ã€‚
        """
        super(Reshape, self).__init__()
        self.shape = shape

    def forward(self, x):
        # `*self.shape` ä¼šè§£åŒ…ä¸ºå¤šç»´æ•°ï¼Œé…åˆ batch size é‡æ–°è°ƒæ•´ tensor å½¢çŠ¶
        return x.view(x.size(0), *self.shape)

    def extra_repr(self):
        # è®© `print(model)` æ—¶æ˜¾ç¤ºé¢å¤–ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•
        # (Optional)Set the extra information about this module. You can test
        # it by printing an object of this class.
        return 'shape={}'.format(
            self.shape
        )


class Decoder(nn.Module):
    """åå·ç§¯è§£ç å™¨ (generator)ã€‚

    å°†çƒé¢éšå˜é‡ *z* æŠ•å½±å›å›¾åƒç©ºé—´ (1Ã—28Ã—28)ã€‚
    - å…ˆå…¨è¿æ¥åˆ°ä¸€ä¸ªé«˜ç»´å‘é‡ï¼Œå† reshape æˆ CNN ç‰¹å¾å›¾ï¼Œ
      æœ€åç» 2 å±‚ ConvTranspose2d å¾—åˆ°é‡æ„å›¾åƒã€‚
    """

    def __init__(self, latent_dim=50, x_shape=(1, 28, 28), cshape=(128, 7, 7), verbose=False):
        super(Decoder, self).__init__()
        # ------------------------- å‚æ•°ç¼“å­˜ -------------------------
        self.latent_dim = latent_dim         # éšç©ºé—´ç»´åº¦ (= Encoder è¾“å‡ºç»´åº¦)
        self.ishape = cshape                  # reshape å CNN feature map å½¢çŠ¶
        self.iels = int(np.prod(self.ishape))  # flatten é•¿åº¦
        self.x_shape = x_shape                 # æœ€ç»ˆè¾“å‡ºå›¾åƒå½¢çŠ¶
        self.output_channels = x_shape[0]
        self.verbose = verbose
        # ------------------------- ä¸»å¹²ç½‘ç»œ -------------------------
        # çº¿æ€§ -> ReLU -> çº¿æ€§ -> ReLU -> Reshape -> åå·ç§¯*2 -> Sigmoid
        # Sigmoid è®©åƒç´ å€¼é™å®šåœ¨ [0, 1]ï¼Œé€‚åˆäºŒå€¼äº¤å‰ç†µ (BCE) é‡æ„è¯¯å·®
        self.model = nn.Sequential(
            # â–º å…¨è¿æ¥é˜¶æ®µ
            nn.Linear(self.latent_dim, 1024),
            nn.ReLU(True),

            nn.Linear(1024, self.iels),
            nn.ReLU(True),
            #
            Reshape(self.ishape),
        )
        self.model = nn.Sequential(
            # â–º å·ç§¯ä¸Šé‡‡æ ·é˜¶æ®µ
            self.model,
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.ReLU(True),
            #
            nn.ConvTranspose2d(64, self.output_channels, 4, stride=2, padding=1),
            nn.Sigmoid(),
        )
         # åˆå§‹åŒ–æƒé‡ï¼ˆKaiming Normal ç­‰ï¼‰ï¼Œä¿æŒè®­ç»ƒç¨³å®š
        init_weights(self)

        if self.verbose:
            print(self.model)

    def forward(self, x):
        """å‚æ•° x ä¸º shape=(B, latent_dim) çš„éšå˜é‡ã€‚"""
        gen_img = self.model(x)
        # è¿”å›å½¢çŠ¶ç»Ÿä¸€çš„å›¾åƒ (B, C, H, W)
        return gen_img.view(x.size(0), *self.x_shape)


class Encoder(nn.Module):
    """å·ç§¯ç¼–ç å™¨ã€‚

    ç»“æ„è®¾è®¡éµå¾ª *DCGAN* é£æ ¼ï¼šConv â†’ ReLU â†’ Conv â†’ ReLU â†’ Flatten â†’ FC â†’ ReLUã€‚
    é¢å¤–è¾“å‡ºï¼š
        - `mu`      : æ–¹å‘å‘é‡ (å½’ä¸€åŒ–åˆ°å•ä½çƒé¢)
        - `k` (Îº)   : é›†ä¸­åº¦ï¼Œsoftplus ä¿è¯éè´Ÿï¼Œå¹¶åŠ å¸¸æ•° r (é»˜è®¤ 80) åšä¸‹é™
        - `z`       : ä» vMF(Î¼, Îº) é‡å‚æ•°åŒ–é‡‡æ ·çš„éšå‘é‡
    """

    def __init__(self, input_channels=1, output_channels=64, cshape=(128, 7, 7), r=80, verbose=False):
        super(Encoder, self).__init__()

        self.cshape = cshape
        self.iels = int(np.prod(self.cshape))
        self.lshape = (self.iels,)
        self.output_channels = output_channels# éšç©ºé—´ç»´åº¦ (= latent_dim64)
        self.input_channels = input_channels
        self.r = r               # Îº ä¸‹é™ï¼Œé˜²æ­¢ collapse
        self.verbose = verbose
        # ------------------------- ä¸»å¹²ç½‘ç»œ -------------------------
        self.model = nn.Sequential(
            nn.Conv2d(self.input_channels, 64, 4, stride=2, padding=1),
            nn.ReLU(True),

            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.ReLU(True),

            Reshape(self.lshape),

            nn.Linear(self.iels, 1024),
            nn.ReLU(True),
        )
        # â–º è¾“å‡º Î¼, Îº åˆ†æ”¯
        self.mu = nn.Linear(1024, self.output_channels)#D ç»´å•ä½å‘é‡ï¼Œè¡¨ç¤ºå¹³å‡æ–¹å‘ï¼›
        self.k = nn.Linear(1024, 1)  #â€ƒâ€¢ Îº (â€œkappaâ€ / â€œkâ€) æ˜¯ æ ‡é‡ï¼Œè¡¨ç¤ºè¯¥æ–¹å‘ä¸Šçš„é›†ä¸­åº¦ï¼ˆÎº è¶Šå¤§ï¼Œæ ·æœ¬è¶Šâ€œæŒ¤â€åœ¨ Î¼ é™„è¿‘ï¼‰ã€‚


        init_weights(self)

        if self.verbose:
            print(self.model)

    def forward(self, x):
        """å‰å‘ä¼ æ’­ã€‚
        è¿”å› (z, mu, kappa)ã€‚
        """
        # ----- 1) å¹³å‡æ–¹å‘ Î¼ -----
        x = self.model(x)
        mu = self.mu(x)
        # ----- 2) é›†ä¸­åº¦ Îº -----
        # softplus ä¿è¯ Îº > 0ï¼›åŠ å¸¸æ•° r åšã€åœ°æ¿ã€é˜²æ­¢æ—©æœŸ Îº å¤ªå¤§ or ä¸º 0
        # We limit kappa to be greater than a certain threshold, because larger kappa will make the cluster more compact.
        k = F.softplus(self.k(x)) + self.r# shape = [B, 1]ï¼Œ æ¯ä¸ªæ ·æœ¬ä¸€ä¸ª Îºï¼Œæ ‡é‡
        # ----- 3) é‡å‚æ•°åŒ–é‡‡æ · z ~ vMF(Î¼, Îº) -----
        # VonMisesFisher ç±»å®ç°äº† `rsample()`ï¼Œå› æ­¤å¯åå‘ä¼ æ’­
        mu = mu / mu.norm(dim=1, keepdim=True) # å•ä½åŒ–ï¼Œè½åœ¨ S^{D-1}
        z = VonMisesFisher(mu, k).rsample()# (B, D)#å¦‚æœä½ æ²¡æœ‰ä¼  shapeï¼Œå®ƒå°±æ˜¯ torch.Size()ï¼Œå³ []è¿™è¡¨ç¤ºå¯¹ æ¯ä¸ªåˆ†å¸ƒé‡‡ 1 ä¸ªæ ·æœ¬

        return z, mu, k


class VMFMM(nn.Module):
    """vMF æ··åˆæ¨¡å‹ (å¯å­¦ä¹ å‚æ•°)ã€‚

    ç›¸å½“äºåœ¨éšç©ºé—´å¼•å…¥ã€èšç±»å…ˆéªŒã€ï¼š
        p(z, c) = Ï€_c * vMF(z; Î¼_c, Îº_c)
    å…¶ä¸­ Ï€_c, Î¼_c, Îº_c ä¸ºå¯å­¦ä¹ çš„ç»„ä»¶å‚æ•°ã€‚
    """

    def __init__(self, n_cluster=10, n_features=10):
        super(VMFMM, self).__init__()

        self.n_cluster = n_cluster # èšç±»æ•° (= K)
        self.n_features = n_features# éšç©ºé—´ç»´åº¦ (= D)
        # ---- å‚æ•°åˆå§‹åŒ– ----
        # Ï€_cï¼šåˆå§‹åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ (1/K)
        # Î¼_cï¼šéšæœºå‘é‡ â†’ å•ä½åŒ–ï¼Œæ¯ç°‡å‡å€¼ # Îº_cï¼šåœ¨åŒºé—´ [1, 5] å‡åŒ€åˆå§‹åŒ–ï¼ˆæ•°å€¼è¿‡å¤§è®­ç»ƒéš¾ï¼Œè¿‡å°åˆ†å¸ƒè¿‡äºæ‰å¹³ï¼‰
        mu = torch.FloatTensor(self.n_cluster, self.n_features).normal_(0, 0.02)
        self.pi_ = nn.Parameter(torch.FloatTensor(self.n_cluster, ).fill_(1) / self.n_cluster, requires_grad=True)
        self.mu_c = nn.Parameter(mu / mu.norm(dim=-1, keepdim=True), requires_grad=True)
        self.k_c = nn.Parameter(torch.FloatTensor(self.n_cluster, ).uniform_(1, 5), requires_grad=True)
    # ------------------------------------------------------------------
    # æ¨æ–­ (Eâ€‘step)ï¼šç»™å®š z è®¡ç®—è´£ä»»æ¦‚ç‡ Î³_icï¼Œå†å– argmax å¾—ç¦»æ•£æ ‡ç­¾
    # ----------------------------------------------------------------
    def predict(self, z):
        """ç»™å®šéšå‘é‡ zï¼Œè¾“å‡ºç¡¬èšç±»æ ‡ç­¾ (numpy array)ã€‚"""
        pi = self.pi_
        mu_c = self.mu_c
        k_c = self.k_c
        yita_c = torch.exp(torch.log(pi.unsqueeze(0)) + self.vmfmm_pdfs_log(z, mu_c, k_c))

        yita = yita_c.detach().cpu().numpy()
        return np.argmax(yita, axis=1)
    # ------------------------------------------------------------------
    # é‡‡æ ·ï¼šæŒ‡å®šæŸç°‡ (ç´¢å¼• k) çš„æ¡ä»¶ä¸‹ç”Ÿæˆæ–¹å‘å‘é‡
    # ------------------------------------------------------------------
    def sample_by_k(self, k, num=10):
        """ä»ç¬¬ k ä¸ªç»„ä»¶ vMF(Î¼_k, Îº_k) é‡‡æ · num ä¸ª zã€‚"""
        mu = self.mu_c[k:k+1]
        k = self.k_c[k].view((1, 1))
        z = None
        for i in range(num):
            _z = VonMisesFisher(mu, k).rsample()
            if z is None:
                z = _z
            else:
                z = torch.cat((z, _z))
        return z
    # ------------------------------------------------------------------
    # è¾…åŠ©å‡½æ•°ï¼šæ‰¹é‡è®¡ç®— log p(z | c=k)å¯¹ zï¼Œè®¡ç®—å®ƒåœ¨æ¯ä¸€ä¸ªvmfåˆ†å¸ƒ ğ‘ğ‘˜ä¸Šçš„å¯¹æ•°æ¦‚ç‡å¯†åº¦ logğ‘(ğ‘§ğ‘–âˆ£ğ‘ğ‘˜)
    # ------------------------------------------------------------------
    def vmfmm_pdfs_log(self, x, mu_c, k_c):

        VMF = []
        for c in range(self.n_cluster):
            VMF.append(self.vmfmm_pdf_log(x, mu_c[c:c + 1, :], k_c[c]).view(-1, 1))
        return torch.cat(VMF, 1)
    
    """å•ç»„ä»¶ vMF å¯¹æ•°å¯†åº¦ã€‚
        å…¬å¼ï¼š
            log f(z) = (D/2 - 1) log Îº - D/2 log Ï€ - log I_{D/2-1}(Îº) + Îº Î¼^T z
    """
    @staticmethod
    def vmfmm_pdf_log(x, mu, k):
        D = x.size(1)
        log_pdf = (D / 2 - 1) * torch.log(k) - D / 2 * math.log(math.pi) - torch.log(besseli(D / 2 - 1, k)) \
                  + x.mm(torch.transpose(mu, 1, 0) * k)#	torch.transpose(mu, 1, 0)(D, 1)	æŠŠå‡å€¼æ–¹å‘ Î¼ ä» (1, D) è½¬æˆåˆ—å‘é‡ï¼Œä¾¿äºåç»­çŸ©é˜µä¹˜ï¼Œx.mm(...)ï¼šæ‰¹é‡è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸ Îº Î¼ çš„ç‚¹ç§¯ã€‚
        return log_pdf
    # ------------------------------------------------------------------
    # å˜åˆ†ä¸‹ç•Œ (ELBO) ä¸­å…³äºèšç±»å…ˆéªŒçš„æœŸæœ›é¡¹ (è´Ÿå·ååšæœ€å°åŒ–)ã€‚
    # ------------------------------------------------------------------
    def vmfmm_Loss(self, z, z_mu, z_k):
        """
        å‚æ•°
        ----------
        z    : (B, D)   é‡‡æ ·éšå˜é‡
        z_mu : (B, D)   Encoder è¾“å‡ºæ–¹å‘å‘é‡
        z_k  : (B, 1)   Encoder è¾“å‡ºé›†ä¸­åº¦

        è¿”å›
        ------
        Loss : torch.Tensor æ ‡é‡ï¼›å€¼è¶Šå°è¶Šå¥½ (å·²å–è´ŸæœŸæœ›)
        """

        det = 1e-10 # é˜²æ­¢ log(0)
        pi = self.pi_         # (K,)
        mu_c = self.mu_c      # (K, D)
        k_c = self.k_c        # (K,)

        D = self.n_features
        # ---------- 1) è´£ä»»æ¦‚ç‡ Î³_ic = q(c|z) ----------
        yita_c = torch.exp(torch.log(pi.unsqueeze(0)) + self.vmfmm_pdfs_log(z, mu_c, k_c)) + det
        yita_c = yita_c / (yita_c.sum(1).view(-1, 1))  # batch_size*Clusters # å½’ä¸€åŒ–åˆ°æ¦‚ç‡

        # ---------- 2) E_q[Îº_c Î¼_c^T z] ----------
        # dI_v/dÎº / I_v(Îº) â‰ˆ E[Î¼^T z]ï¼Œå‚è§ vMF çš„æœŸæœ›æ€§è´¨
        # batch * n_cluster
        e_k_mu_z = (d_besseli(D / 2 - 1, z_k) * z_mu).mm((k_c.unsqueeze(1) * mu_c).transpose(1, 0)) # (B, K)
        
        # ---------- 3) E_q[Îº_z Î¼_z^T z] (selfâ€‘term) --------
        # batch * 1
        e_k_mu_z_new = torch.sum((d_besseli(D / 2 - 1, z_k) * z_mu) * (z_k * z_mu), 1, keepdim=True)

        # e_log_z_x
        Loss = torch.mean((D * ((D / 2 - 1) * torch.log(z_k) - D / 2 * math.log(math.pi) - torch.log(besseli(D / 2 - 1, z_k)) + e_k_mu_z_new)))

        # e_log_z_c
        Loss -= torch.mean(torch.sum(yita_c * (
                D * ((D / 2 - 1) * torch.log(k_c) - D / 2 * math.log(math.pi) - torch.log(besseli(D / 2 - 1, k_c)) + e_k_mu_z)), 1))

        Loss -= torch.mean(torch.sum(yita_c * torch.log(pi.unsqueeze(0) / yita_c), 1))
        return Loss
