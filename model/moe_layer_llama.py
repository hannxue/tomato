import torch
import torch.nn as nn
import torch.nn.init as init
import math
from typing import List, Optional, Tuple
import torch.nn.functional as F
from transformers import LlamaConfig
from megablocks.layers.moe import batched_load_balancing_loss, clear_load_balancing_loss, get_load_balancing_loss

class SparseTop2MLP(nn.Module):
    def __init__(self, config, intermediate_size = 256):#NoneåŸæ¥ffn=intermediate_size=14336->SILU->4096
        super().__init__()                               #ç°åœ¨intermediate_size=2048
        
        self.ffn_dim = intermediate_size
        self.hidden_dim = config.hidden_size

        self.f1 = nn.Linear(self.hidden_dim, self.ffn_dim,    bias = False)#f1: å°†è¾“å…¥ä» hidden_dim æŠ•å½±åˆ° ffn_dimã€‚
        self.f2 = nn.Linear(self.ffn_dim,    self.hidden_dim, bias = False)#f2: æ˜¯è¾“å‡ºå±‚ï¼ŒæŠŠ ffn_dim â†’ hidden_dimï¼Œç”¨äºè¿˜åŸåŸå§‹ç»´åº¦ã€‚
        self.f3 = nn.Linear(self.hidden_dim, self.ffn_dim,    bias = False)#f3: ä¹Ÿæ˜¯ hidden_dim â†’ ffn_dimï¼Œä½†ç”¨äºæ„é€ é—¨æ§åˆ†æ”¯ã€‚

        self.act = nn.SiLU()# transformer ä¸­å¸¸ç”¨çš„éçº¿æ€§å‡½æ•°SiLU(x)=xâ‹…sigmoid(x)

    def forward(self, hidden_state):#hidden_state æ˜¯æ¥è‡ªä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œå½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, seq_len, hidden_dim]

        x = self.act(self.f1(hidden_state) * self.f3(hidden_state))
        x = self.f2(x)
        return x

class MoeBlock_RS(nn.Module):

    def __init__(self, config, cluster_index_list, dataset_num, moe_config):
        super().__init__()
        self.ffn_dim = 1280            # æ¯ä¸ªä¸“å®¶å†…éƒ¨éšå±‚ (åŸ 14336ï¼Œè¿™é‡Œé™ç»´èŠ‚çœç®—åŠ›)
        self.hidden_dim = config.hidden_size  # Llama-3 hidden size, åœ¨ Llama-3 8B é‡Œï¼ŒTransformer çš„ä¸»éšè—ç»´åº¦ hidden_size å°±æ˜¯ 4096->6ä¸ªä¸“å®¶(åœ¨æ˜¾å­˜èŒƒå›´å†…)
        self.num_experts = 12                       # K = 12 ä¸ªä¸“å®¶,å› ä¸º4096æ”¹æˆ2048äº†
        self.top_k = 2                              # æ¯ä¸ª token åªèµ° 2 ä¸ªä¸“å®¶ (ç¨€ç–è·¯ç”±)
        self.num_cluster = 2                        # ç”¨æˆ·-ç‰©å“èšç±»æ•°é‡ = 5
        
        # ------------------------ 1ï¸âƒ£ é—¨æ§ (Gate) ----------------
        #   gate0, gate1, â€¦ gate4 åˆ†åˆ«å¯¹åº” 5 ä¸ªèšç±»çš„ç‹¬ç«‹è½¯è·¯ç”±å™¨
        #   æ¯ä¸ªé—¨æ§ï¼šhidden â†’ num_experts logits
        self.gate = nn.ModuleDict({f"gate{i}": nn.Linear(self.hidden_dim, self.num_experts, bias = False) for i in range(self.num_cluster)})
        # ------------------------ 2ï¸âƒ£ ä¸“å®¶ (Experts) --------------
        #   è¿™é‡Œç”¨ä½œè€…å®šä¹‰çš„ SparseTop2MLPï¼šå†…éƒ¨æ˜¯ feed-forward MLP
        self.experts = nn.ModuleList([SparseTop2MLP(config) for _ in range(self.num_experts)])
        # ------------------------ 3ï¸âƒ£ è¿è¡Œæ—¶è¾…åŠ©å˜é‡ --------------
        self.cluster_index_list = cluster_index_list    # æ¯æ¡æ ·æœ¬æ‰€å±èšç±»ï¼ˆç¦»çº¿é¢„è®¡ç®—ï¼‰
        self.foward_count = 0   
        self.cluster_index_count = 0                    # æ¸¸æ ‡ï¼šæŒ‡å‘å½“å‰æ ·æœ¬çš„èšç±»ç´¢å¼•æ¸¸æ ‡ / æŒ‡é’ˆï¼Œè®°å½•â€œå½“å‰ MoE å±‚å·²ç»å¤„ç†åˆ°ç¬¬å‡ æ¡æ ·æœ¬â€
        self.dataset_num = dataset_num                  # æ•°æ®é›†å¤§å°ï¼ˆå¯åšå½’ä¸€åŒ–ç­‰ï¼‰
        self.moe_config = moe_config
    '''
        input: 
            hidden_state: Transformer FFN
            cluster_index: index for choose which gate to use
                ex: cluster 0 : gate 0
                    cluster 1 : gate 1
                    ...
                    cluster n : gate n
                shape: [batch_size]
    '''
    def get_capacity(self, num_tokens: int, num_experts: int, capacity_factor: float, min_capacity=None):
        capacity = math.ceil((num_tokens / num_experts) * capacity_factor)
        if min_capacity is not None and capacity < min_capacity:
            capacity = min_capacity
        return capacity
    # -------------------------------------------------------------------------
    #  forward(hidden_states)  
    #     hidden_states: (B, L, H) â€”â€” Transformer çš„ token è¡¨ç¤º
    #hidden_states å¹¶ä¸æ˜¯åœ¨ MoeBlock_RS é‡Œç”Ÿæˆçš„ï¼Œè€Œæ˜¯ ä½œä¸ºå‚æ•°ç”±ä¸Šä¸€å±‚ Transformer ä¼ è¿›æ¥çš„ã€‚
    # -------------------------------------------------------------------------
    def forward(self, hidden_states: torch.Tensor):
        batch_size, seq_len, hidden_dim = hidden_states.shape

        router_logits_list = []
        # ---------- 1. è®¡ç®—å½“å‰ batch æ¯ä¸ªæ ·æœ¬çš„è·¯ç”± logits ----------
        for i,idx in enumerate(range(batch_size)):
            # â¶ å–æœ¬æ ·æœ¬åœ¨èšç±»åˆ—è¡¨ä¸­çš„ç´¢å¼•
            gate_index = self.cluster_index_list[self.cluster_index_count-1]#åœ¨è¿›å…¥ for i in range(batch_size): å¾ªç¯ä¹‹å‰ï¼Œæ¸¸æ ‡å·²ç»è¢« +1ã€‚
            # â· ç”¨å¯¹åº” gate_i ç”Ÿæˆ (L, H) @ (H, num_experts)->(L, num_experts) logits;ç”¨å¯¹åº”çš„é—¨æ§ gateX æŠŠè¯¥æ ·æœ¬çš„ æ¯ä¸ª token è¡¨ç¤º æ˜ å°„æˆ num_experts ä¸ªåˆ†æ•°ã€‚ shape=(L,12)
            #æ¥æ”¶å½“å‰ token çš„éšè—å‘é‡ï¼Œè¾“å‡ºå¯¹ å…¨éƒ¨ä¸“å®¶ çš„æ‰“åˆ†ï¼Œå†æŒ‘è‹¥å¹²ä¸ªæœ€åˆé€‚çš„ä¸“å®¶å»å¤„ç†ã€‚
            routing_logits = self.gate['gate{}'.format(gate_index)](hidden_states[i])
            router_logits_list.append(routing_logits)
        # concat â†’ (B*L, num_experts),æ–¹ä¾¿ä¸€æ¬¡æ€§å¯¹æ‰€æœ‰ token åš softmax/topk
        router_logits = torch.stack(router_logits_list).view(-1, self.num_experts)
        # ğŸ” ä¿å­˜è·¯ç”± logitsï¼Œç”¨äºåç»­å¤–éƒ¨è®¡ç®— z-lossï¼ˆæ¯”å¦‚åœ¨ Vmoe_llama3.forward ä¸­ï¼‰
        #ä¸ºä»€ä¹ˆç”¨ .detach()ï¼Ÿï¼Œé¿å…ä¸»æŸå¤±å’Œ z-loss é‡å¤åå‘ä¼ æ’­æ¢¯åº¦ï¼Œä½ åªéœ€è¦å®ƒçš„å€¼åšæ­£åˆ™é¡¹ï¼Œä¸å¸Œæœ›å®ƒå½±å“ä¸»è®¡ç®—å›¾
        self.router_logits = router_logits
        # reshape hidden ä¸º (B*L, H) æ–¹ä¾¿é€ token å¤„ç†
        hidden_states = hidden_states.view(-1, hidden_dim)
        # ---------- 2. softmax å¾—åˆ°è·¯ç”±æ¦‚ç‡ ----------
        routing_probs = F.softmax(router_logits, dim=-1)
        # ---------- 3. å– Top-2 ä¸“å®¶ ----------
        # select top2 experts
        routing_weights, selected_experts = torch.topk(routing_probs, self.top_k, dim = -1)   # (B*L, 2)
        # fusing weight && add  å½’ä¸€åŒ–æƒé‡ï¼Œä½¿ä¸¤ä¸“å®¶æƒé‡ç›¸åŠ  =1
        routing_weights = routing_weights / torch.sum(routing_weights, dim = -1, keepdim = True).to(hidden_states.dtype)
        # ---------- 4. å‡†å¤‡è¾“å‡ºå ä½ tensor -------
        #  init maxtrix to save resultï¼Œå…¨é›¶ å¼ é‡ï¼Œshape=(BÂ·L, H)
        final_hidden_states = torch.zeros(
            (batch_size * seq_len, hidden_dim),dtype=hidden_states.dtype,device=hidden_states.device
        )
        # ---------- 5. one-hot æ©ç ï¼ŒæŠŠ token åˆ†é…åˆ°å¯¹åº”ä¸“å®¶ ----------
        # expert_mask: (num_experts, top_k, B*L)
        # for efficiency, calculate the result one time using the mask
        expert_mask = nn.functional.one_hot(selected_experts, num_classes = self.num_experts)
        #expert_mask[e]  (top_k=2, BÂ·L=20)
         #r=0  â–’ 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 â–’
         #r=1  â–’ 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 â–’
        #torch.where â†’ idx=[0,1, â€¦]   top_x=[1,3, â€¦]
        # [20,2,8] ---> [8,2,20],è¿™é‡Œçš„8åº”è¯¥æ”¹æˆ12ï¼Œå› ä¸ºæœ‰12ä¸ªä¸“å®¶
        expert_mask = expert_mask.permute(2, 1, 0)

        # --- Compute token capacity ---
        num_tokens = batch_size * seq_len
        capacity = self.get_capacity(
            num_tokens=num_tokens,
            num_experts=self.num_experts,
            capacity_factor=getattr(self.moe_config, "capacity_factor", 1.25),
            min_capacity=getattr(self.moe_config, "min_capacity", None)
        )

         # for loadâ€‘balancing stats
        tokens_per_expert = torch.zeros(self.num_experts, dtype=torch.int32, device=hidden_states.device)
        # ---------- 6. é€ä¸“å®¶è®¡ç®—ï¼Œå¹¶æŠŠç»“æœç´¯åŠ åˆ° final_hidden ----------
        for expert_index in range(self.num_experts):
            expert_layer = self.experts[expert_index]
            idx, top_x = torch.where(expert_mask[expert_index])# å“ªäº› token é€‰äº†è¯¥ä¸“å®¶
            top_x_list = top_x.tolist()                   # å¯¹åº”åœ¨ hidden_states çš„è¡Œå·
            idx_list = idx.tolist()                       # 0 æˆ– 1ï¼ˆä¸¤æ¡è·¯ç”±ä¸­çš„å“ªä¸ªï¼‰
            # token dropping if exceeding capacity
            if len(top_x_list) > capacity:
                top_x_list = top_x_list[:capacity]
                idx_list = idx_list[:capacity]

           

            # ğŸ‘‡ æ³¨æ„ä¸‹é¢è¿™äº›éƒ½è¦åŸºäºæˆªæ–­åçš„ top_x_list å’Œ idx_list
            top_x_tensor = torch.tensor(top_x_list, device=hidden_states.device,dtype=torch.long)
            idx_tensor = torch.tensor(idx_list, device=hidden_states.device,dtype=torch.long)
             # stats BEFORE weighting (to match Megablocks spec)
            tokens_per_expert[expert_index] = len(top_x_list)#å®é™…åˆ†åˆ°ä¸“å®¶ e çš„ token ä¸ªæ•°
            current_state = hidden_states[top_x_tensor].reshape(-1, hidden_dim)
            #current_state = hidden_states[None,top_x_tensor].reshape(-1, hidden_dim)  # æ”¶é›†å±äºè¯¥ä¸“å®¶çš„ä¸€å † token è¡¨ç¤ºï¼Œshape (Náµ—, H)ã€‚
            #â‘  æ‰¹é‡ç»è¿‡ä¸“å®¶ MLPï¼›# routing_weights[top_x_list,idx_list, None] å–å¯¹åº”çš„æ¦‚ç‡ wâ‚€ï¼Œunsqueeze æˆ (Náµ—, 1)ï¼Œæ‰èƒ½ä¸ (Náµ—, H) åšé€è¡Œä¹˜æ³•
            #â‘¡ ä¹˜ä¸Šè·¯ç”±æ¦‚ç‡åšåŠ æƒã€‚
            current_hidden_states = expert_layer(current_state) * routing_weights[top_x_tensor,idx_tensor, None] # ä¸“å®¶å‰å‘
            current_hidden_states = current_hidden_states.to(hidden_states.dtype)
            #- å¦‚æœä¸€ä¸ª token èµ°äº† 2 ä¸ªä¸“å®¶ï¼Œindex_add_ ä¼šåœ¨åŒä¸€è¡ŒæŠŠä¸¤è·¯ç»“æœç›¸åŠ ï¼ˆç¬¦åˆè®ºæ–‡é‡Œçš„ åŠ æƒæ±‚å’Œï¼‰ã€‚
            #ä¾‹å­output_token3 = wâ‚€ Â· yâ‚€ + wâ‚ Â· yâ‚= 0.7Â·[10,10]  +  0.3Â·[20,20]= [13, 13]
            final_hidden_states.index_add_(0, top_x_tensor, current_hidden_states)#dimï¼šåœ¨å“ªä¸ªç»´åº¦åšç´¯åŠ ï¼Œè¿™é‡Œ 0 = è¡Œï¼Œindexï¼šè¡Œ/åˆ—ç´¢å¼•å¼ é‡ï¼Œsourceï¼šè¦åŠ è¿›å»çš„å¼ é‡ï¼Œå½¢çŠ¶ä¸ index å¯¹åº”ç»´åº¦é•¿åº¦ä¸€è‡´
        final_hidden_states = final_hidden_states.reshape(batch_size,seq_len,hidden_dim)
        #load_balancing_loss = batched_load_balancing_loss(self.moe_config)
        # åªæœ‰åœ¨çœŸæ­£éœ€è¦æ¢¯åº¦çš„é‚£ä¸€æ¬¡ forward é‡Œæ‰è®°å½•ï¼Œ é¿å… gradient-checkpointing äº§ç”ŸåŒå€æ¡ç›®ã€‚
        if self.training:
             get_load_balancing_loss().append((tokens_per_expert, routing_probs))
        
        
        return final_hidden_states


if __name__ == '__main__':
    import random 
    config = LlamaConfig()
    cluster_index_list = [random.randint(0, 4) for _ in range(10)]
    print(f'index{cluster_index_list}')
    moe_block = MoeBlock_RS(config, cluster_index_list)
    test_tensor = torch.randn(5,10,4096)
    out = moe_block(test_tensor)
    # print(out)
    print(out.shape)
